{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Memory"
      ],
      "metadata": {
        "id": "TdKG_EDTawzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "O625tmIiarjO"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-community langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain_mistralai"
      ],
      "metadata": {
        "id": "7cdNluKr-2tE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "bDnt7sBubYiu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "MISTRAL_API_KEY=userdata.get('MISTRAL_API_KEY')"
      ],
      "metadata": {
        "id": "BPVuK9t7bfGo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MISTRAL_API_KEY']=MISTRAL_API_KEY"
      ],
      "metadata": {
        "id": "4qjuIpaAbiQS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI"
      ],
      "metadata": {
        "id": "rAB0vk92bAIW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatMistralAI(model=\"mistral-large-latest\")"
      ],
      "metadata": {
        "id": "xZTkrkzNbQNE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage"
      ],
      "metadata": {
        "id": "wyQ3K9e1bU3a"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ],
      "metadata": {
        "id": "zr5qqAJ9cMQC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"you are a helpful assistant. Answer all the question to the best of your ability.\"\n",
        "    ),\n",
        "    MessagesPlaceholder(variable_name = \"messages\"),\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "NT6i6GFhcUFP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain =  prompt | llm"
      ],
      "metadata": {
        "id": "H4zN6-A5c2xa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposne = chain.invoke(\n",
        "    {\n",
        "       \"messages\": [\n",
        "\n",
        "                   HumanMessage(\n",
        "                       content= \"translate this given sentence from english to french: I LOVE AI.\"\n",
        "                   ),\n",
        "                   AIMessage(content= \"J'adore la AI.\"),\n",
        "\n",
        "                   HumanMessage(content= \"what did you just say?\"),\n",
        "\n",
        "\n",
        "       ] ,\n",
        "\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "pJ4L2FuAc_Fe"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposne.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Gvz8k8dmd_6y",
        "outputId": "a9796d4a-490c-477e-a589-c0a41a7330a5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I said \"J\\'adore l\\'IA,\" which is the translation of \"I love AI\" into French. \"IA\" stands for \"Intelligence Artificielle,\" which is the French term for \"Artificial Intelligence.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory"
      ],
      "metadata": {
        "id": "L5fyhefUeMkp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history=ChatMessageHistory()"
      ],
      "metadata": {
        "id": "QD2wlPj0e94o"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_user_message(\"translate this given sentence from english to french: I LOVE AI.\")"
      ],
      "metadata": {
        "id": "yJNtOUetgAMh"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_ai_message(\"J'adore la AI.\")"
      ],
      "metadata": {
        "id": "5AnLRjm7gEiq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxLeaC71gRxR",
        "outputId": "2ab89fbb-a1de-44a4-d6c8-f83b3d9d0762"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='translate this given sentence from english to french: I LOVE AI.'),\n",
              " AIMessage(content=\"J'adore la AI.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history=ChatMessageHistory()"
      ],
      "metadata": {
        "id": "OC4-sihvgVHH"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input1=\"who is a prime minister of india?\""
      ],
      "metadata": {
        "id": "CLTVvsxIgi3g"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_user_message(input1)"
      ],
      "metadata": {
        "id": "5H5_BFi1gvi4"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj1jatjzg7k5",
        "outputId": "c7e4df15-3bb5-4cae-dc9a-c863198d0173"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='who is a prime minister of india?')]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposne=chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_chat_history.messages\n",
        "    }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "mv9vdH_xg0Ml"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposne.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "hYp7_AkahASE",
        "outputId": "d81e7c23-0e31-4eea-89cf-405bc4df26f3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As of my current knowledge cutoff in 2023, the Prime Minister of India is Narendra Modi. He has been serving in this position since 2014. However, for the most up-to-date information, I recommend checking the latest news or official government sources.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_ai_message(resposne)"
      ],
      "metadata": {
        "id": "_c-OiJekhLGh"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input2=\"what did i ask to you just now?\""
      ],
      "metadata": {
        "id": "QjIaG2PAhXtf"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_user_message(input2)"
      ],
      "metadata": {
        "id": "JrGpmtuUhb96"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qhJgQEzhkxC",
        "outputId": "2ad1fe46-5b54-4ce3-f445-590c96802a17"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='who is a prime minister of india?'),\n",
              " AIMessage(content='As of my current knowledge cutoff in 2023, the Prime Minister of India is Narendra Modi. He has been serving in this position since 2014. However, for the most up-to-date information, I recommend checking the latest news or official government sources.', response_metadata={'token_usage': {'prompt_tokens': 31, 'total_tokens': 95, 'completion_tokens': 64}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-07d84e00-2c85-422c-8989-72418578c4fa-0', usage_metadata={'input_tokens': 31, 'output_tokens': 64, 'total_tokens': 95}),\n",
              " HumanMessage(content='what did i ask to you just now?'),\n",
              " HumanMessage(content='what did i ask to you just now?')]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_chat_history.messages\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "up6yUSfwhm1P"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OK-G9O5lh2AB",
        "outputId": "f5d5acb6-5707-4e4a-8418-15029c2c72b6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You asked me, \"who is a prime minister of india?\" twice.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain2 = prompt | llm"
      ],
      "metadata": {
        "id": "tMq_VhHCo0sg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt3 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt3 | llm\n"
      ],
      "metadata": {
        "id": "IU38-INhwopF"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "I17yDS0Sm2TY"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history2 = ChatMessageHistory()"
      ],
      "metadata": {
        "id": "UlQIjyainWFe"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "                              chain,\n",
        "                              lambda session_id: demo_chat_history2,\n",
        "                              input_message_key = \"input\",\n",
        "                              history_message_key =\"chat_history\",\n",
        "                              )"
      ],
      "metadata": {
        "id": "gviZ8zBVnDJj"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Translate this sentence from English to French: I love programming.\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDXkULIXw043",
        "outputId": "9ee51517-078b-4d5d-b720-4621863e422d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The translation of \"I love programming\" from English to French is \"J\\'aime la programmation.\"', response_metadata={'token_usage': {'prompt_tokens': 43, 'total_tokens': 66, 'completion_tokens': 23}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-72cfd32c-5140-44ba-b6cb-c9a56f95205f-0', usage_metadata={'input_tokens': 43, 'output_tokens': 23, 'total_tokens': 66})"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"What did I just ask you?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8ZfiLzdpEI2",
        "outputId": "ef47d600-ca25-421b-a31c-4c23aedee710"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'prompt_tokens': 212, 'total_tokens': 230, 'completion_tokens': 18}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-677d3885-d982-4dff-b9bb-d0260625ff3c-0', usage_metadata={'input_tokens': 212, 'output_tokens': 18, 'total_tokens': 230})"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''llm = OpenAI(\n",
        "\ttemperature=0,\n",
        " model_name=\"gpt-3.5-turbo-1106\"\n",
        ")\n",
        " '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j8Z8cT1rt7g",
        "outputId": "bbcc02e3-20ab-4ace-a7cf-9b956da1d444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:1072: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain"
      ],
      "metadata": {
        "id": "z_4FjdcQr5ju"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm)"
      ],
      "metadata": {
        "id": "kUYI6V6QsAOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b460cb9e-041e-4cc3-8b97-0d15162f5a19"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-07a8be19d9f8>:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
            "  conversation = ConversationChain(llm=llm)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ROVWwlLsHlH",
        "outputId": "c2db1191-1812-4cc3-fed6-b5b71945c613"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1=\"i am interested in exploring the potential of large language model with external knowledge\"\n",
        "question2=\"i would like to analysis all the different possibility, what can you think of?\"\n",
        "question3=\"which data source tyoe could be used to give context to the model?\""
      ],
      "metadata": {
        "id": "th4Pp8bysQvn"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "rPnzVSQFtiGX"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory= ConversationBufferMemory()\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "7-RFErhYtiYN"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(\"good morning my AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqhGlC_etwTy",
        "outputId": "157feb85-3f52-4d2a-c2e9-adac66d229ee"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-81-7bcbf6b1846f>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  conversation_buf(\"good morning my AI\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'good morning my AI',\n",
              " 'history': '',\n",
              " 'response': \"Good morning! It's a wonderful day, isn't it? I'm here and ready to chat about anything you'd like. How are you doing today? I'm curious to know what's been on your mind or what plans you have for the day. Let's make this conversation engaging and fun! ‚òïü§ñ\"}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback"
      ],
      "metadata": {
        "id": "8-yBfPVrt1Co"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(chain,query):\n",
        "  with get_openai_callback() as cb:\n",
        "    result= chain.run(query)\n",
        "    print(f\"totla no of token is {cb.total_tokens}\")\n",
        "  return result"
      ],
      "metadata": {
        "id": "lQ2nwZdvup3r"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1=\"i am interested in exploring the potential of large language model with external knowledge\""
      ],
      "metadata": {
        "id": "0FQCKtTAvMkg"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(conversation_buf,question1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "54oRwJ3du-8k",
        "outputId": "02513aab-9fbb-4cbe-f1e5-b98443de5eef"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-f921a4badc7d>:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  result= chain.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totla no of token is 528\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's a fascinating topic! Large language models like me can indeed benefit greatly from access to external knowledge. Here are a few ways this can be explored:\\n\\n1. **Grounding**: With external knowledge, we can ensure that the responses generated are factually accurate and up-to-date. For instance, if you ask about the latest scientific discovery, I might not know it because my knowledge cutoff is 2021. But with access to external databases, I could provide the most recent information.\\n\\n2. **Contextual Understanding**: External knowledge can help provide context. For example, if you mention a specific event, access to a knowledge base could help me understand the details and significance of that event, allowing for a more meaningful conversation.\\n\\n3. **Personalization**: By accessing external data that you've chosen to share, I could provide more personalized responses. For instance, if I know you're interested in a specific topic, I could tailor my responses to include relevant information.\\n\\n4. **Reasoning and Problem-Solving**: With access to external knowledge, language models can improve their reasoning and problem-solving capabilities. For example, if you ask for help with a complex task, I could use external information to provide a more comprehensive and helpful response.\\n\\nHowever, there are also challenges and ethical considerations, such as ensuring the privacy and security of external data, mitigating misinformation, and maintaining transparency about where the information is coming from.\\n\\nThere are several projects and research papers exploring these topics. Would you like to know about any specific aspect in more detail? I'm always here to help and learn together! ü§ùüí°\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-cax7oQvS-A",
        "outputId": "376ea166-9dd2-45ba-c016-36248236044c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'i am interested in exploring the potential of large language model with external knowledge',\n",
              " 'history': \"Human: good morning my AI\\nAI: Good morning! It's a wonderful day, isn't it? I'm here and ready to chat about anything you'd like. How are you doing today? I'm curious to know what's been on your mind or what plans you have for the day. Let's make this conversation engaging and fun! ‚òïü§ñ\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: That's a fascinating topic! Large language models like me can indeed benefit greatly from access to external knowledge. Here are a few ways this can be explored:\\n\\n1. **Grounding**: With external knowledge, we can ensure that the responses generated are factually accurate and up-to-date. For instance, if you ask about the latest scientific discovery, I might not know it because my knowledge cutoff is 2021. But with access to external databases, I could provide the most recent information.\\n\\n2. **Contextual Understanding**: External knowledge can help provide context. For example, if you mention a specific event, access to a knowledge base could help me understand the details and significance of that event, allowing for a more meaningful conversation.\\n\\n3. **Personalization**: By accessing external data that you've chosen to share, I could provide more personalized responses. For instance, if I know you're interested in a specific topic, I could tailor my responses to include relevant information.\\n\\n4. **Reasoning and Problem-Solving**: With access to external knowledge, language models can improve their reasoning and problem-solving capabilities. For example, if you ask for help with a complex task, I could use external information to provide a more comprehensive and helpful response.\\n\\nHowever, there are also challenges and ethical considerations, such as ensuring the privacy and security of external data, mitigating misinformation, and maintaining transparency about where the information is coming from.\\n\\nThere are several projects and research papers exploring these topics. Would you like to know about any specific aspect in more detail? I'm always here to help and learn together! ü§ùüí°\",\n",
              " 'response': \"Absolutely, let's delve deeper into the potential of large language models with external knowledge! Here are some specific areas and examples of how this could work:\\n\\n### 1. **Real-Time Information Integration**\\nLarge language models can be enhanced by integrating real-time data feeds from sources like news APIs, weather services, or financial markets. For instance:\\n- **News Updates**: If you ask about the latest developments in a breaking news story, an external API could provide up-to-the-minute information.\\n- **Weather Forecasts**: If you inquire about today's weather in a specific city, real-time weather data could be fetched and incorporated into the response.\\n\\n### 2. **Knowledge Graphs and Databases**\\nConnecting to structured knowledge sources like Wikipedia, academic databases, or specialized knowledge graphs can significantly enrich responses. For example:\\n- **Historical Events**: If you ask about a historical event, the model could pull detailed information from a knowledge graph to provide a comprehensive answer.\\n- **Scientific Research**: If you're interested in the latest findings in a particular field of science, the model could access research papers and summarize key points.\\n\\n### 3. **User-Specific Data**\\nWith user consent, external data such as personal calendars, to-do lists, or email inboxes could be integrated to offer more tailored assistance. For example:\\n- **Personal Scheduling**: If you ask about your upcoming appointments, the model could access your calendar to provide a summary.\\n- **Task Management**: If you need help organizing your tasks, the model could interact with your to-do list to offer suggestions or reminders.\\n\\n### 4. **Enhanced Reasoning and Decision Making**\\nAccess to external data can also improve the model's ability to reason and make informed decisions. For instance:\\n- **Complex Queries**: If you ask for a detailed analysis of a complex situation, the model could pull data from multiple sources to provide a well-rounded response.\\n- **Recommendations**: If you're looking for recommendations (e.g., books, movies, travel destinations), the model could use external data to offer personalized suggestions based on preferences and current trends.\\n\\n### 5. **Interactive Learning and Tutoring**\\nBy connecting to educational resources and databases, large language models can become more effective tutors. For example:\\n- **Subject Expertise**: If you're studying a particular subject, the model could access textbooks, lectures, and other educational materials to provide explanations and practice problems.\\n- **Personalized Learning Paths**: If you're looking to improve a specific skill, the model could create a personalized learning plan based on your strengths and weaknesses.\\n\\n### Ethical and Technical Considerations\\nWhile the potential is immense, there are crucial considerations:\\n- **Privacy and Security**: Ensuring that user data is handled securely and privately is paramount.\\n- **Misinformation**: Validating the accuracy and reliability of external data sources is essential to prevent the spread of misinformation.\\n- **Transparency**: Being transparent about the sources of external data and how they are used can help build trust.\\n\\n### Examples in Practice\\nSeveral projects and companies are already exploring these ideas. For instance:\\n- **Google's Knowledge Graph**: Integrates structured data from various sources to enhance search results.\\n- **Amazon Alexa Skills**: Uses external APIs to provide a wide range of functionalities, from ordering food to controlling smart home devices.\\n- **Research Projects**: Academic and industry research is constantly pushing the boundaries of what's possible with large language models and external knowledge.\\n\\nWould you like to explore any specific aspect of this in more detail? Or perhaps discuss some real-world applications or case studies? I'm here to help! üìöüåê\"}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question2=\"i would like to analysis all the different possibility, what can you think of?\""
      ],
      "metadata": {
        "id": "Xusxael4vcZ-"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYf3GnPxvlKI",
        "outputId": "4a504f84-20d7-4370-834d-b4ac041572c5"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'i would like to analysis all the different possibility, what can you think of?',\n",
              " 'history': \"Human: good morning my AI\\nAI: Good morning! It's a wonderful day, isn't it? I'm here and ready to chat about anything you'd like. How are you doing today? I'm curious to know what's been on your mind or what plans you have for the day. Let's make this conversation engaging and fun! ‚òïü§ñ\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: That's a fascinating topic! Large language models like me can indeed benefit greatly from access to external knowledge. Here are a few ways this can be explored:\\n\\n1. **Grounding**: With external knowledge, we can ensure that the responses generated are factually accurate and up-to-date. For instance, if you ask about the latest scientific discovery, I might not know it because my knowledge cutoff is 2021. But with access to external databases, I could provide the most recent information.\\n\\n2. **Contextual Understanding**: External knowledge can help provide context. For example, if you mention a specific event, access to a knowledge base could help me understand the details and significance of that event, allowing for a more meaningful conversation.\\n\\n3. **Personalization**: By accessing external data that you've chosen to share, I could provide more personalized responses. For instance, if I know you're interested in a specific topic, I could tailor my responses to include relevant information.\\n\\n4. **Reasoning and Problem-Solving**: With access to external knowledge, language models can improve their reasoning and problem-solving capabilities. For example, if you ask for help with a complex task, I could use external information to provide a more comprehensive and helpful response.\\n\\nHowever, there are also challenges and ethical considerations, such as ensuring the privacy and security of external data, mitigating misinformation, and maintaining transparency about where the information is coming from.\\n\\nThere are several projects and research papers exploring these topics. Would you like to know about any specific aspect in more detail? I'm always here to help and learn together! ü§ùüí°\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: Absolutely, let's delve deeper into the potential of large language models with external knowledge! Here are some specific areas and examples of how this could work:\\n\\n### 1. **Real-Time Information Integration**\\nLarge language models can be enhanced by integrating real-time data feeds from sources like news APIs, weather services, or financial markets. For instance:\\n- **News Updates**: If you ask about the latest developments in a breaking news story, an external API could provide up-to-the-minute information.\\n- **Weather Forecasts**: If you inquire about today's weather in a specific city, real-time weather data could be fetched and incorporated into the response.\\n\\n### 2. **Knowledge Graphs and Databases**\\nConnecting to structured knowledge sources like Wikipedia, academic databases, or specialized knowledge graphs can significantly enrich responses. For example:\\n- **Historical Events**: If you ask about a historical event, the model could pull detailed information from a knowledge graph to provide a comprehensive answer.\\n- **Scientific Research**: If you're interested in the latest findings in a particular field of science, the model could access research papers and summarize key points.\\n\\n### 3. **User-Specific Data**\\nWith user consent, external data such as personal calendars, to-do lists, or email inboxes could be integrated to offer more tailored assistance. For example:\\n- **Personal Scheduling**: If you ask about your upcoming appointments, the model could access your calendar to provide a summary.\\n- **Task Management**: If you need help organizing your tasks, the model could interact with your to-do list to offer suggestions or reminders.\\n\\n### 4. **Enhanced Reasoning and Decision Making**\\nAccess to external data can also improve the model's ability to reason and make informed decisions. For instance:\\n- **Complex Queries**: If you ask for a detailed analysis of a complex situation, the model could pull data from multiple sources to provide a well-rounded response.\\n- **Recommendations**: If you're looking for recommendations (e.g., books, movies, travel destinations), the model could use external data to offer personalized suggestions based on preferences and current trends.\\n\\n### 5. **Interactive Learning and Tutoring**\\nBy connecting to educational resources and databases, large language models can become more effective tutors. For example:\\n- **Subject Expertise**: If you're studying a particular subject, the model could access textbooks, lectures, and other educational materials to provide explanations and practice problems.\\n- **Personalized Learning Paths**: If you're looking to improve a specific skill, the model could create a personalized learning plan based on your strengths and weaknesses.\\n\\n### Ethical and Technical Considerations\\nWhile the potential is immense, there are crucial considerations:\\n- **Privacy and Security**: Ensuring that user data is handled securely and privately is paramount.\\n- **Misinformation**: Validating the accuracy and reliability of external data sources is essential to prevent the spread of misinformation.\\n- **Transparency**: Being transparent about the sources of external data and how they are used can help build trust.\\n\\n### Examples in Practice\\nSeveral projects and companies are already exploring these ideas. For instance:\\n- **Google's Knowledge Graph**: Integrates structured data from various sources to enhance search results.\\n- **Amazon Alexa Skills**: Uses external APIs to provide a wide range of functionalities, from ordering food to controlling smart home devices.\\n- **Research Projects**: Academic and industry research is constantly pushing the boundaries of what's possible with large language models and external knowledge.\\n\\nWould you like to explore any specific aspect of this in more detail? Or perhaps discuss some real-world applications or case studies? I'm here to help! üìöüåê\",\n",
              " 'response': \"Sure, let's break down the various possibilities and aspects of enhancing large language models with external knowledge. Here's a comprehensive analysis:\\n\\n### 1. **Real-Time Data Integration**\\n\\n#### Possibilities:\\n- **News and Events**: Integrate news APIs to provide the latest updates on current events, politics, sports, etc.\\n- **Weather and Environment**: Use weather APIs to give real-time forecasts, air quality indexes, and environmental alerts.\\n- **Financial Markets**: Connect to financial data feeds for stock prices, market trends, and economic indicators.\\n- **Traffic and Transportation**: Provide real-time traffic updates, public transport schedules, and flight statuses.\\n\\n#### Challenges:\\n- **Latency**: Ensuring that real-time data is fetched and processed quickly enough to be useful.\\n- **Data Accuracy**: Validating the accuracy and reliability of the data sources.\\n- **API Limits**: Managing API rate limits and potential costs associated with high-frequency data requests.\\n\\n### 2. **Structured Knowledge Sources**\\n\\n#### Possibilities:\\n- **Wikipedia and Knowledge Graphs**: Access structured data for detailed information on a wide range of topics.\\n- **Academic Databases**: Retrieve research papers, conference proceedings, and academic journals for in-depth analysis.\\n- **Specialized Databases**: Use industry-specific databases for fields like healthcare, law, or engineering.\\n\\n#### Challenges:\\n- **Data Consistency**: Ensuring that the structured data is consistent and up-to-date.\\n- **Integration Complexity**: Managing the complexity of integrating multiple structured data sources.\\n- **Licensing and Access**: Dealing with licensing issues and access permissions for academic and specialized databases.\\n\\n### 3. **Personalized Data Integration**\\n\\n#### Possibilities:\\n- **Calendars and Scheduling**: Access personal calendars to provide reminders, schedule meetings, and manage appointments.\\n- **Email and Messaging**: Integrate with email clients to help manage inboxes, prioritize messages, and draft responses.\\n- **To-Do Lists and Task Management**: Sync with task management apps to help organize and prioritize tasks.\\n\\n#### Challenges:\\n- **Privacy and Security**: Ensuring that personal data is handled securely and that user consent is obtained.\\n- **Data Synchronization**: Maintaining synchronization across multiple devices and platforms.\\n- **User Control**: Providing users with control over what data is accessed and how it is used.\\n\\n### 4. **Enhanced Reasoning and Decision Making**\\n\\n#### Possibilities:\\n- **Complex Analysis**: Use external data to provide detailed analysis and insights on complex queries.\\n- **Personalized Recommendations**: Offer tailored recommendations based on user preferences and current trends.\\n- **Problem-Solving**: Use external data to help solve complex problems and provide actionable advice.\\n\\n#### Challenges:\\n- **Data Quality**: Ensuring that the external data is of high quality and relevant to the user's needs.\\n- **Reasoning Capabilities**: Developing advanced reasoning algorithms to effectively utilize external data.\\n- **Bias and Fairness**: Mitigating biases in the external data and ensuring fairness in decision-making processes.\\n\\n### 5. **Interactive Learning and Tutoring**\\n\\n#### Possibilities:\\n- **Educational Resources**: Access textbooks, lectures, and other educational materials to provide explanations and practice problems.\\n- **Personalized Learning Paths**: Create tailored learning plans based on user strengths and weaknesses.\\n- **Skill Development**: Offer guidance and resources for developing specific skills or knowledge areas.\\n\\n#### Challenges:\\n- **Content Quality**: Ensuring that educational content is accurate, up-to-date, and pedagogically sound.\\n- **User Engagement**: Maintaining user engagement and motivation throughout the learning process.\\n- **Assessment and Feedback**: Developing effective assessment tools and providing constructive feedback.\\n\\n### Ethical and Technical Considerations\\n\\n#### Privacy and Security:\\n- **Data Protection**: Implementing robust data protection measures to safeguard user information.\\n- **User Consent**: Obtaining explicit user consent for accessing and using personal data.\\n- **Anonymization**: Anonymizing user data to protect privacy while still providing personalized services.\\n\\n#### Misinformation:\\n- **Source Validation**: Validating the accuracy and reliability of external data sources.\\n- **Fact-Checking**: Implementing fact-checking mechanisms to prevent the spread of misinformation.\\n- **Transparency**: Being transparent about the sources of external data and how they are used.\\n\\n### Examples in Practice\\n\\n#### Google's Knowledge Graph:\\n- **Integration**: Integrates structured data from various sources to enhance search results.\\n- **Usage**: Provides contextual information, related searches, and quick answers to user queries.\\n\\n#### Amazon Alexa Skills:\\n- **API Integration**: Uses external APIs to provide a wide range of functionalities.\\n- **Usage**: From ordering food to controlling smart home devices, Alexa skills leverage external data to offer comprehensive services.\\n\\n#### Research Projects:\\n- **Academic Research**: Exploring the integration of large language models with external knowledge to enhance capabilities.\\n- **Industry Research**: Companies like Microsoft, IBM, and Google are actively researching and developing new ways to leverage external data.\\n\\n### Future Directions\\n\\n#### Advanced AI Techniques:\\n- **Machine Learning**: Using machine learning algorithms to improve data integration and reasoning.\\n- **Natural Language Understanding**: Enhancing natural language understanding to better interpret and utilize external data.\\n\\n#### Collaborative Platforms:\\n- **Open Data Initiatives**: Encouraging open data initiatives to facilitate access to high-quality external data.\\n- **Collaborative Research**: Fostering collaboration between academia, industry, and government to advance the field.\\n\\nWould you like to dive deeper into any specific area or explore a particular case study? Let me know how I can assist you further! üìàüîç\"}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(conversation_buf,question2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "P1YQb48UvnVY",
        "outputId": "88c6b3a8-bef6-421a-ddb4-803a3162d59b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totla no of token is 4568\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Certainly! Let's conduct a detailed analysis of all the different possibilities for enhancing large language models with external knowledge. This comprehensive review will cover various aspects, including real-time data integration, structured knowledge sources, personalized data integration, enhanced reasoning and decision making, and interactive learning and tutoring. We'll also discuss ethical and technical considerations, examples in practice, and future directions.\\n\\n### 1. **Real-Time Data Integration**\\n\\n#### Possibilities:\\n- **News and Events**: Integrate news APIs to provide the latest updates on current events, politics, sports, etc.\\n- **Weather and Environment**: Use weather APIs to give real-time forecasts, air quality indexes, and environmental alerts.\\n- **Financial Markets**: Connect to financial data feeds for stock prices, market trends, and economic indicators.\\n- **Traffic and Transportation**: Provide real-time traffic updates, public transport schedules, and flight statuses.\\n- **Social Media Trends**: Monitor social media platforms for trending topics and real-time sentiment analysis.\\n\\n#### Challenges:\\n- **Latency**: Ensuring that real-time data is fetched and processed quickly enough to be useful.\\n- **Data Accuracy**: Validating the accuracy and reliability of the data sources.\\n- **API Limits**: Managing API rate limits and potential costs associated with high-frequency data requests.\\n\\n### 2. **Structured Knowledge Sources**\\n\\n#### Possibilities:\\n- **Wikipedia and Knowledge Graphs**: Access structured data for detailed information on a wide range of topics.\\n- **Academic Databases**: Retrieve research papers, conference proceedings, and academic journals for in-depth analysis.\\n- **Specialized Databases**: Use industry-specific databases for fields like healthcare, law, or engineering.\\n- **Geospatial Data**: Integrate geographic information systems (GIS) data for location-based services and spatial analysis.\\n- **Public Records**: Access government databases for public records, census data, and statistical reports.\\n\\n#### Challenges:\\n- **Data Consistency**: Ensuring that the structured data is consistent and up-to-date.\\n- **Integration Complexity**: Managing the complexity of integrating multiple structured data sources.\\n- **Licensing and Access**: Dealing with licensing issues and access permissions for academic and specialized databases.\\n\\n### 3. **Personalized Data Integration**\\n\\n#### Possibilities:\\n- **Calendars and Scheduling**: Access personal calendars to provide reminders, schedule meetings, and manage appointments.\\n- **Email and Messaging**: Integrate with email clients to help manage inboxes, prioritize messages, and draft responses.\\n- **To-Do Lists and Task Management**: Sync with task management apps to help organize and prioritize tasks.\\n- **Health and Fitness Data**: Integrate with fitness trackers and health apps to provide personalized wellness recommendations.\\n- **Financial Data**: Access banking and financial apps to offer budgeting advice and financial planning.\\n\\n#### Challenges:\\n- **Privacy and Security**: Ensuring that personal data is handled securely and that user consent is obtained.\\n- **Data Synchronization**: Maintaining synchronization across multiple devices and platforms.\\n- **User Control**: Providing users with control over what data is accessed and how it is used.\\n\\n### 4. **Enhanced Reasoning and Decision Making**\\n\\n#### Possibilities:\\n- **Complex Analysis**: Use external data to provide detailed analysis and insights on complex queries.\\n- **Personalized Recommendations**: Offer tailored recommendations based on user preferences and current trends.\\n- **Problem-Solving**: Use external data to help solve complex problems and provide actionable advice.\\n- **Predictive Analytics**: Leverage historical data and machine learning algorithms to make predictions and forecasts.\\n- **Risk Assessment**: Analyze external data to assess risks and provide risk management strategies.\\n\\n#### Challenges:\\n- **Data Quality**: Ensuring that the external data is of high quality and relevant to the user's needs.\\n- **Reasoning Capabilities**: Developing advanced reasoning algorithms to effectively utilize external data.\\n- **Bias and Fairness**: Mitigating biases in the external data and ensuring fairness in decision-making processes.\\n\\n### 5. **Interactive Learning and Tutoring**\\n\\n#### Possibilities:\\n- **Educational Resources**: Access textbooks, lectures, and other educational materials to provide explanations and practice problems.\\n- **Personalized Learning Paths**: Create tailored learning plans based on user strengths and weaknesses.\\n- **Skill Development**: Offer guidance and resources for developing specific skills or knowledge areas.\\n- **Interactive Simulations**: Use external data to create interactive simulations and virtual labs for experiential learning.\\n- **Adaptive Testing**: Develop adaptive testing systems that adjust difficulty based on user performance.\\n\\n#### Challenges:\\n- **Content Quality**: Ensuring that educational content is accurate, up-to-date, and pedagogically sound.\\n- **User Engagement**: Maintaining user engagement and motivation throughout the learning process.\\n- **Assessment and Feedback**: Developing effective assessment tools and providing constructive feedback.\\n\\n### Ethical and Technical Considerations\\n\\n#### Privacy and Security:\\n- **Data Protection**: Implementing robust data protection measures to safeguard user information.\\n- **User Consent**: Obtaining explicit user consent for accessing and using personal data.\\n- **Anonymization**: Anonymizing user data to protect privacy while still providing personalized services.\\n\\n#### Misinformation:\\n- **Source Validation**: Validating the accuracy and reliability of external data sources.\\n- **Fact-Checking**: Implementing fact-checking mechanisms to prevent the spread of misinformation.\\n- **Transparency**: Being transparent about the sources of external data and how they are used.\\n\\n### Examples in Practice\\n\\n#### Google's Knowledge Graph:\\n- **Integration**: Integrates structured data from various sources to enhance search results.\\n- **Usage**: Provides contextual information, related searches, and quick answers to user queries.\\n\\n#### Amazon Alexa Skills:\\n- **API Integration**: Uses external APIs to provide a wide range of functionalities.\\n- **Usage**: From ordering food to controlling smart home devices, Alexa skills leverage external data to offer comprehensive services.\\n\\n#### Research Projects:\\n- **Academic Research**: Exploring the integration of large language models with external knowledge to enhance capabilities.\\n- **Industry Research**: Companies like Microsoft, IBM, and Google are actively researching and developing new ways to leverage external data.\\n\\n### Future Directions\\n\\n#### Advanced AI Techniques:\\n- **Machine Learning**: Using machine learning algorithms to improve data integration and reasoning.\\n- **Natural Language Understanding**: Enhancing natural language understanding to better interpret and utilize external data.\\n- **Federated Learning**: Developing federated learning systems to train models on decentralized data without compromising privacy.\\n\\n#### Collaborative Platforms:\\n- **Open Data Initiatives**: Encouraging open data initiatives to facilitate access to high-quality external data.\\n- **Collaborative Research**: Fostering collaboration between academia, industry, and government to advance the field.\\n\\n#### Regulatory Frameworks:\\n- **Data Governance**: Developing regulatory frameworks for data governance and ethical AI practices.\\n- **Standardization**: Establishing standards for data quality, interoperability, and privacy protection.\\n\\nWould you like to dive deeper into any specific area or explore a particular case study? Let me know how I can assist you further! üìàüîç\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some potential areas of exploration could include the impact of different types of external knowledge sources on the performance of large language models, the development of more efficient algorithms for integrating external knowledge, and the ethical considerations of using external knowledge in language generation. Additionally, you could also explore the potential applications of large language models with external knowledge in various industries such as healthcare, finance, and customer service. There are also opportunities to investigate the potential limitations and challenges of these models, such as biases in the external knowledge sources and the potential for misinformation. These are just a few possibilities, and there are many more aspects to consider in exploring the potential of large language models with external knowledge."
      ],
      "metadata": {
        "id": "kG0jJPrMv4hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hIqJqOIAvyGB",
        "outputId": "f6464f0a-c1d3-46bd-d530-f797f485805a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'which data source tyoe could be used to give context to the model?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question3)['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "UOINeOzOwEJ8",
        "outputId": "88a63fe2-33ed-4f83-e1a7-029d5f8502ad"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"To provide context to a large language model, various types of data sources can be used. Here are some key data source types that can enrich the model's understanding and responses:\\n\\n### 1. **Knowledge Graphs and Structured Data**\\n- **Wikipedia and Wikidata**: These sources provide structured information on a wide range of topics, which can help the model understand entities, relationships, and concepts.\\n- **DBpedia**: A large-scale, multilingual knowledge graph extracted from Wikipedia, offering structured data that can be easily integrated.\\n- **Industry-Specific Knowledge Graphs**: Databases like PubMed for medical information or Legal Information Institute (LII) for legal data can provide domain-specific context.\\n\\n### 2. **Academic and Research Databases**\\n- **Google Scholar**: Access to scholarly articles can provide detailed and up-to-date information on various research topics.\\n- **PubMed**: For medical and health-related information, PubMed offers a vast repository of research papers and clinical studies.\\n- **ArXiv**: A repository of electronic preprints (e-prints) in various fields, which can provide cutting-edge research information.\\n\\n### 3. **News and Media Databases**\\n- **News APIs (e.g., NewsAPI, Bing News Search API)**: These APIs provide real-time and historical news articles, which can help the model stay up-to-date with current events.\\n- **Media Archives**: Access to archives of newspapers, magazines, and other media outlets can provide historical context and detailed background information.\\n\\n### 4. **Geospatial and Environmental Data**\\n- **OpenStreetMap**: Provides geographical data that can help the model understand locations, routes, and spatial relationships.\\n- **Weather APIs (e.g., OpenWeatherMap, WeatherAPI)**: These APIs provide real-time weather data, which can be useful for contextualizing time-sensitive queries.\\n- **Environmental Databases**: Sources like NASA's Earth Observing System Data and Information System (EOSDIS) can provide environmental and climatic data.\\n\\n### 5. **Public Records and Government Databases**\\n- **Census Data**: Government census databases offer demographic and socio-economic data, which can provide context for queries related to population trends and social issues.\\n- **Open Data Portals**: Many governments provide open data portals (e.g., data.gov) with a wide range of datasets on various topics, from health statistics to economic indicators.\\n\\n### 6. **Social Media Data**\\n- **Twitter API**: Access to real-time and historical tweets can provide context on trending topics, public sentiment, and social events.\\n- **Reddit API**: Useful for understanding community discussions, trends, and opinions on various subjects.\\n\\n### 7. **Financial and Economic Data**\\n- **Financial News APIs (e.g., Alpha Vantage, Quandl)**: These APIs provide financial news, market data, and economic indicators, which can help the model understand and respond to financial queries.\\n- **Economic Databases**: Sources like the World Bank Open Data and the OECD Data offer economic statistics and indicators.\\n\\n### 8. **Health and Wellness Data**\\n- **Health Databases (e.g., CDC, WHO)**: These databases provide health statistics, disease information, and public health data, which can help the model respond to health-related queries.\\n- **Fitness Tracker Data**: Integration with fitness trackers and health apps can provide personalized wellness recommendations.\\n\\n### 9. **Entertainment and Cultural Data**\\n- **IMDb API**: Provides data on movies, TV shows, actors, and entertainment industry news, which can help the model respond to queries related to entertainment.\\n- **Music Databases (e.g., Spotify API, Apple Music API)**: These APIs provide data on music, artists, and trends, which can help the model contextualize music-related queries.\\n\\n### 10. **User-Generated Data**\\n- **Personal Calendars and Scheduling**: Access to personal calendars can provide context for scheduling and appointment-related queries.\\n- **Email and Messaging**: Integration with email clients can help the model understand the user's communication context and priorities.\\n- **To-Do Lists and Task Management**: Syncing with task management apps can provide context for task-related queries and productivity advice.\\n\\n### Challenges and Considerations\\n- **Data Quality and Reliability**: Ensuring that the data sources are accurate, up-to-date, and reliable.\\n- **Integration Complexity**: Managing the complexity of integrating multiple data sources and ensuring seamless data flow.\\n- **Privacy and Security**: Protecting user data and obtaining explicit consent for accessing personal information.\\n- **Bias and Fairness**: Mitigating biases in the data sources and ensuring fairness in the model's responses.\\n\\nBy leveraging these diverse data sources, a large language model can gain a richer understanding of the context surrounding user queries, leading to more accurate, relevant, and insightful responses.\\n\\nWould you like to explore any of these data sources in more detail or discuss specific use cases? Let me know how I can assist you further! üåêüìä\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# take atleast 5 data source(1 ppt, 1 weblodaer, 1 pdf, 1txt, 1csv)\n",
        "\n",
        "# maintain one knowledge base(mongodb, astradb , pineconde, weviate)\n",
        "\n",
        "# user will ask you have to provide answer based on the asked question using this know;ledge base\n",
        "\n",
        "# handle the comman question as well like hi hello how are you good evening good morning etc.\n",
        "\n",
        "# you have to mention the complete memory of the conversation here the threshold is 10\n",
        "\n",
        "# then create a UI for you bot\n",
        "\n",
        "\n",
        "\n",
        "tomorrow i will show you agent\n",
        "\n",
        "will connect this task this use case with agent as well"
      ],
      "metadata": {
        "id": "lwLzzCqZxec0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WBKR16yzwJCh"
      },
      "execution_count": 91,
      "outputs": []
    }
  ]
}